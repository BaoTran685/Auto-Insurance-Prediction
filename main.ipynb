{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:27.948957Z",
     "iopub.status.busy": "2024-10-16T21:51:27.948572Z",
     "iopub.status.idle": "2024-10-16T21:51:27.955115Z",
     "shell.execute_reply": "2024-10-16T21:51:27.953788Z",
     "shell.execute_reply.started": "2024-10-16T21:51:27.948920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go # note that github wont be able to display these plots because they are interactive\n",
    "# for some processing\n",
    "import math\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:27.987014Z",
     "iopub.status.busy": "2024-10-16T21:51:27.986510Z",
     "iopub.status.idle": "2024-10-16T21:51:27.995044Z",
     "shell.execute_reply": "2024-10-16T21:51:27.993844Z",
     "shell.execute_reply.started": "2024-10-16T21:51:27.986946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "class SuppressPrints:\n",
    "  def __enter__(self):\n",
    "    self._original_stdout = sys.stdout  # Save the original stdout\n",
    "    sys.stdout = open(os.devnull, 'w')  # Redirect stdout to null\n",
    "\n",
    "  def __exit__(self, exc_type, exc_value, traceback):\n",
    "    sys.stdout.close()  # Close the null file\n",
    "    sys.stdout = self._original_stdout  # Restore original stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.028854Z",
     "iopub.status.busy": "2024-10-16T21:51:28.028467Z",
     "iopub.status.idle": "2024-10-16T21:51:28.043814Z",
     "shell.execute_reply": "2024-10-16T21:51:28.042490Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.028816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.091739Z",
     "iopub.status.busy": "2024-10-16T21:51:28.090611Z",
     "iopub.status.idle": "2024-10-16T21:51:28.164602Z",
     "shell.execute_reply": "2024-10-16T21:51:28.163471Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.091684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "folder = \"kaggle/input/\"\n",
    "df = pd.read_csv(folder + \"train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.166746Z",
     "iopub.status.busy": "2024-10-16T21:51:28.166440Z",
     "iopub.status.idle": "2024-10-16T21:51:28.186314Z",
     "shell.execute_reply": "2024-10-16T21:51:28.185204Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.166714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = df.drop(columns = [\"CustomerID\", \"Coverage\", \"Education\", \"Employment Status\", \"Marital Status\", \"Policy Type\", \"Policy\", \"Sales Channel\", \"Vehicle Size\"])\n",
    "df_train.info()\n",
    "df_train[\"Claim over 1k\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.286178Z",
     "iopub.status.busy": "2024-10-16T21:51:28.285261Z",
     "iopub.status.idle": "2024-10-16T21:51:28.293084Z",
     "shell.execute_reply": "2024-10-16T21:51:28.291906Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.286117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def numeric_info(df, col):\n",
    "  print(f\"Min of {col}: \", df[col].min())\n",
    "  print(f\"Max of {col}: \", df[col].max())\n",
    "  print(f\"Mean of {col}: \", df[col].mean())\n",
    "  print(f\"Mendian of {col}: \", df[col].median())\n",
    "  print(f\"Std of {col}: \", df[col].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.412462Z",
     "iopub.status.busy": "2024-10-16T21:51:28.412055Z",
     "iopub.status.idle": "2024-10-16T21:51:28.420163Z",
     "shell.execute_reply": "2024-10-16T21:51:28.418918Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.412421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numeric_info(df_train, \"Customer Lifetime Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.496593Z",
     "iopub.status.busy": "2024-10-16T21:51:28.496101Z",
     "iopub.status.idle": "2024-10-16T21:51:28.509107Z",
     "shell.execute_reply": "2024-10-16T21:51:28.507931Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.496542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def relation_to_claim_over_1k(df):\n",
    "  plt.figure(figsize=(30, 80))\n",
    "  numerical_cols = df.columns.tolist()\n",
    "  not_include = [\"Customer Lifetime Value\", \"Income\", \"Claim over 1k\"]\n",
    "  numerical_cols = [col for col in numerical_cols if col not in not_include]\n",
    "  \n",
    "  number_of_rows = math.ceil(len(numerical_cols) / 2)\n",
    "  fig = make_subplots(rows = number_of_rows, cols=2, subplot_titles=numerical_cols, vertical_spacing=0.01, horizontal_spacing=0.05)\n",
    "  for idx, col in enumerate(numerical_cols):\n",
    "    df_count = df.groupby([col, \"Claim over 1k\"]).size().reset_index(name='count')\n",
    "    unique_val = df_count[col].unique()\n",
    "    i, j = (idx // 2) + 1, (idx % 2) + 1\n",
    "    for val in unique_val:\n",
    "      df_cur = df_count[df_count[col] == val]\n",
    "      fig.add_trace(\n",
    "        go.Bar(\n",
    "          x = df_cur[\"Claim over 1k\"],\n",
    "          y = df_cur[\"count\"],\n",
    "          name = f\"{val}\"\n",
    "        ),\n",
    "        row=i, col=j\n",
    "      )\n",
    "  fig.update_layout(\n",
    "      height=300 * number_of_rows,  # Adjust figure height dynamically based on the number of rows\n",
    "      showlegend=False,\n",
    "      title_text=\"How Each Column Affects 'Claim over 1k'\"\n",
    "  )\n",
    "  # Show the figure with all subplots\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(df, col, size):\n",
    "  df[col + \"_bin\"] = pd.qcut(df[col], q=size, labels=False, duplicates=\"drop\")\n",
    "\n",
    "def convert(n, range_max, range_min):\n",
    "  return n / (range_max - range_min)\n",
    "\n",
    "def pre_process(df):\n",
    "  # State\n",
    "  unique_states = df[\"State\"].unique()\n",
    "  dict_states = {}\n",
    "  for i in range(len(unique_states)):\n",
    "    dict_states[unique_states[i]] = i\n",
    "  df[\"State\"] = df[\"State\"].apply(lambda x: dict_states[x])\n",
    "  \n",
    "  # Response\n",
    "  # note that resonse should be either yes or no\n",
    "  df[\"Response\"] = df[\"Response\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "\n",
    "  # Effective To Date\n",
    "  df[\"Effective To Date\"] = pd.to_datetime(df[\"Effective To Date\"])\n",
    "  df[\"Effective To Season\"] = (df[\"Effective To Date\"].dt.month - 1) // 3 # split into 4 seasons\n",
    "  df.drop(\"Effective To Date\", axis=1, inplace=True)\n",
    "\n",
    "  # Months Since Last Claim, Months Since Last Inception\n",
    "  binning(df, \"Months Since Last Claim\", 10)\n",
    "  binning(df, \"Months Since Policy Inception\", 10)\n",
    "\n",
    "  # Gender\n",
    "  df[\"Gender\"] = df[\"Gender\"].apply(lambda x: 1 if x == \"M\" else 0)\n",
    "\n",
    "  # CLV\n",
    "  binning(df, \"Customer Lifetime Value\", 50)\n",
    "  # Categorize CLV\n",
    "  clv_bins = [df['Customer Lifetime Value'].min(), 5000, 10000, df['Customer Lifetime Value'].max()]\n",
    "  clv_labels = [0, 1, 2]\n",
    "  df['CLV_Category'] = pd.cut(df['Customer Lifetime Value'], bins=clv_bins, labels=clv_labels, include_lowest=True)\n",
    "  \n",
    "  # Income\n",
    "  binning(df, \"Income\", 30)\n",
    "  # Categorize Income\n",
    "  income_bins = [df['Income'].min(), 40000, 80000, df['Income'].max()]\n",
    "  income_labels = [0, 1, 2]\n",
    "  df['Income_Category'] = pd.cut(df['Income'], bins=income_bins, labels=income_labels, include_lowest=True)\n",
    "\n",
    "  # New col: CLV per number of policies and months since policy inception\n",
    "  df[\"Money Spent per Policy\"] = df[\"Customer Lifetime Value\"] / ((df[\"Coverage Index\"] + 1) * (df[\"Number of Policies\"] + 1))\n",
    "\n",
    "  binning(df, \"Money Spent per Policy\", 50)\n",
    "\n",
    "  # New col: Accident Likelihood\n",
    "  max_education_index = df[\"Education Index\"].max()\n",
    "  min_education_index = df[\"Education Index\"].min()\n",
    "  max_income_bin = df[\"Income_bin\"].max()\n",
    "  min_income_bin = df[\"Income_bin\"].min()\n",
    "  max_marital_status_index = df[\"Marital Status Index\"].max()\n",
    "  min_marital_status_index = df[\"Marital Status Index\"].min()\n",
    "  df[\"Accident Likelihood\"] = np.exp( 4 * convert(max_education_index - df[\"Education Index\"], max_education_index, min_education_index) \\\n",
    "    + 3 * convert(max_income_bin - df[\"Income_bin\"], max_income_bin, min_income_bin) \\\n",
    "    + 3 * (df[\"Marital Status Index\"].apply(lambda x: max_marital_status_index if x == 0 else 0)))\n",
    "\n",
    "  binning(df, \"Accident Likelihood\", 10)\n",
    "\n",
    "  # New col: Claim over 1k Likelihood: based on State and Gender\n",
    "  group_over = [\"State\", \"Gender\", \"Employment Status Index\", \"Income_bin\"]\n",
    "  likelihood_df = df.groupby(group_over)[\"Accident Likelihood\"].mean().reset_index()\n",
    "  likelihood_df.rename(columns={\"Accident Likelihood\": \"Location Based Likelihood\"}, inplace=True)\n",
    "  # Merge the likelihood values back to the original dataframe\n",
    "  df = pd.merge(df, likelihood_df, on=group_over, how=\"left\")\n",
    "  \n",
    "  binning(df, \"Location Based Likelihood\", 5)\n",
    "\n",
    "  # New col: Insurance: may relate to the Customer Lifetime Value\n",
    "  df[\"Insurance\"] = np.exp(convert(df[\"Number of Policies\"], df[\"Number of Policies\"].max(), df[\"Number of Policies\"].min()) \\\n",
    "    + convert(df[\"Coverage Index\"], df[\"Coverage Index\"].max(), df[\"Coverage Index\"].min()) \\\n",
    "    + convert(df[\"Number of Policies\"], df[\"Number of Policies\"].max(), df[\"Number of Policies\"].min()) * convert(df[\"Coverage Index\"], df[\"Coverage Index\"].max(), df[\"Coverage Index\"].min())\n",
    "  )\n",
    "\n",
    "  binning(df, \"Insurance\", 40)\n",
    "\n",
    "  # New col: Customer Interaction: may relate to Cusomter Lifetime Value\n",
    "  df[\"Customer Interaction\"] = np.exp(convert(df[\"Response\"], df[\"Response\"].max(), df[\"Response\"].min()) \\\n",
    "    + convert(df[\"Number of Open Complaints\"], df[\"Number of Open Complaints\"].max(), df[\"Number of Open Complaints\"].min()))\n",
    "  \n",
    "  binning(df, \"Customer Interaction\", 30)\n",
    "\n",
    "  # New col: Date Time: may relate to Accident Likelihood\n",
    "  group_datetime = [\"Months Since Last Claim\", \"Effective To Season\"]\n",
    "  datetime_df = df.groupby(group_datetime)[\"Accident Likelihood\"].mean().reset_index()\n",
    "  datetime_df.rename(columns={\"Accident Likelihood\": \"Date Time\"}, inplace=True)\n",
    "  # Merge\n",
    "  df = pd.merge(df, datetime_df, on=group_datetime, how=\"left\")\n",
    "\n",
    "  binning(df, \"Date Time\", 30)\n",
    "\n",
    "  # cols_to_remove = [\"Customer Lifetime Value\", \"Accident Likelihood\", \"Income\", \"Insurance\", \"Location Based Likelihood\", \"Date Time\", \\\n",
    "  #   \"Months Since Policy Inception\", \"Vehicle Size Index\", \"Gender\", \"Effective To Season\", \"Policy Index\", \"Customer Interaction\", \\\n",
    "  #   \"Education Index\", \"Months Since Last Claim\", \"Renew Offer Type\", \"State\", \"Number of Open Complaints\", \"Sales Channel Index\", \"Response\", \"Policy Type Index\"\n",
    "  #   ]\n",
    "  # df.drop(columns=cols_to_remove, axis=1, inplace=True)\n",
    "  # cols_to_use = [\"Customer Lifetime Value\", \"Money Spent per Policy\", \"Accident Likelihood\", \"Number of Policies\", \"Income\", \"Coverage Index\", \\\n",
    "  #   \"Employment Status Index\", \"Insurance\", \"Marital Status Index\", \"Location Based Likelihood\", \"Claim over 1k\", \\\n",
    "  #   \"Date Time\", \"Education Index\", \"Months Since Last Claim\", \"Vehicle Size Index\", \"Gender\", \"Months Since Policy Inception\"\n",
    "  #   ]\n",
    "  \n",
    "  cols_to_use = [\"Customer Lifetime Value\", \"Money Spent per Policy_bin\", \"Accident Likelihood_bin\", \"Number of Policies\", \"Income_bin\", \"Coverage Index\", \\\n",
    "    \"Employment Status Index\", \"Insurance_bin\", \"Marital Status Index\", \"Location Based Likelihood_bin\", \\\n",
    "    \"Date Time\", \"Months Since Last Claim\", \"Gender\", \"Months Since Policy Inception\", \\\n",
    "    \"CLV_Category\", \"Income_Category\"\n",
    "    ]\n",
    "  if (\"Claim over 1k\" in df.columns.tolist()):\n",
    "    cols_to_use.append(\"Claim over 1k\")\n",
    "  return df[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.612524Z",
     "iopub.status.busy": "2024-10-16T21:51:28.612110Z",
     "iopub.status.idle": "2024-10-16T21:51:28.623533Z",
     "shell.execute_reply": "2024-10-16T21:51:28.622379Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.612486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numeric_info(df_train, \"Customer Lifetime Value\")\n",
    "numeric_info(df_train, \"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.665861Z",
     "iopub.status.busy": "2024-10-16T21:51:28.665477Z",
     "iopub.status.idle": "2024-10-16T21:51:28.719248Z",
     "shell.execute_reply": "2024-10-16T21:51:28.718028Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.665823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_process = pre_process(df_train.copy())\n",
    "df_process.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:28.729734Z",
     "iopub.status.busy": "2024-10-16T21:51:28.729269Z",
     "iopub.status.idle": "2024-10-16T21:51:29.110558Z",
     "shell.execute_reply": "2024-10-16T21:51:29.109127Z",
     "shell.execute_reply.started": "2024-10-16T21:51:28.729687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "relation_to_claim_over_1k(df_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def shap_plot(df_train, df_test):\n",
    "  X = df_train.drop(\"Claim over 1k\", axis=1)\n",
    "  y = df_train[\"Claim over 1k\"]\n",
    "  test_size = 0.2\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "\n",
    "  # Train a Random Forest model\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X_train, y_train)\n",
    "  # Initialize the SHAP explainer\n",
    "  explainer = shap.Explainer(model, feature_names=X.columns)\n",
    "  \n",
    "  # Calculate SHAP values for the test dataset\n",
    "  shap_values = explainer(X_test)\n",
    "  return shap_values\n",
    "# shap_values = shap_plot(df_process, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shap_values.shape)\n",
    "# shap_values_0 = shap_values[:, :, 0]\n",
    "# shap_values_1 = shap_values[:, :, 1]\n",
    "# shap.plots.bar(shap_values_0, max_display=400)\n",
    "# shap.plots.bar(shap_values_1, max_display=400)\n",
    "# print(shap_values_0.shape, shap_values_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:29.113170Z",
     "iopub.status.busy": "2024-10-16T21:51:29.112703Z",
     "iopub.status.idle": "2024-10-16T21:51:29.118950Z",
     "shell.execute_reply": "2024-10-16T21:51:29.117733Z",
     "shell.execute_reply.started": "2024-10-16T21:51:29.113119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "def apply_scaler(df, scaler):\n",
    "  scaled_data = scaler.fit_transform(df)\n",
    "  df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "  return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:29.120946Z",
     "iopub.status.busy": "2024-10-16T21:51:29.120524Z",
     "iopub.status.idle": "2024-10-16T21:51:29.506276Z",
     "shell.execute_reply": "2024-10-16T21:51:29.505261Z",
     "shell.execute_reply.started": "2024-10-16T21:51:29.120899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "relation_to_claim_over_1k(apply_scaler(df_process, standard_scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:30.069219Z",
     "iopub.status.busy": "2024-10-16T21:51:30.068668Z",
     "iopub.status.idle": "2024-10-16T21:51:30.076938Z",
     "shell.execute_reply": "2024-10-16T21:51:30.075903Z",
     "shell.execute_reply.started": "2024-10-16T21:51:30.069166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_monitor(model, X, y):\n",
    "  threshold = 0.5\n",
    "  # Predict the test set\n",
    "  y_proba = model.predict_proba(X)\n",
    "  y_pred = (y_proba[:, 1] >= threshold).astype(int)\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y, y_pred)\n",
    "  conf_matrix = confusion_matrix(y, y_pred).T\n",
    "  class_report = classification_report(y, y_pred)\n",
    "\n",
    "  # Calculate accuracy and F1 score\n",
    "  print(f\"Accuracy: {accuracy}\")\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(conf_matrix)\n",
    "  print(\"Classification Report:\")\n",
    "  print(class_report)\n",
    "  f1 = f1_score(y, y_pred)\n",
    "  print(f\"f1 score: ${f1}\")\n",
    "  \n",
    "  #            Actual\n",
    "  #             0   1\n",
    "  # Predict 0   TP  FP\n",
    "  #         1   FN  TN\n",
    "  # Evaluate the model\n",
    "  tp, fp, fn, tn = conf_matrix.ravel()\n",
    "  # Calculate sensitivity and specificity\n",
    "  sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "  specificity = tn / (tn + fp)  # True Negative Rate\n",
    "\n",
    "  print(\"Sensitivity (Recall):\", sensitivity)\n",
    "  print(\"Specificity:\", specificity)\n",
    "\n",
    "  return accuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:30.079387Z",
     "iopub.status.busy": "2024-10-16T21:51:30.078479Z",
     "iopub.status.idle": "2024-10-16T21:51:30.092275Z",
     "shell.execute_reply": "2024-10-16T21:51:30.091170Z",
     "shell.execute_reply.started": "2024-10-16T21:51:30.079338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, scaler):\n",
    "  return scaler.fit_transform(X)\n",
    "\n",
    "def get_train_params(X, y, scaler, test_size):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "  X_train_scaled = normalize(X_train, scaler)\n",
    "  X_test_scaled = normalize(X_test, scaler)\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:30.094121Z",
     "iopub.status.busy": "2024-10-16T21:51:30.093719Z",
     "iopub.status.idle": "2024-10-16T21:51:30.105033Z",
     "shell.execute_reply": "2024-10-16T21:51:30.104121Z",
     "shell.execute_reply.started": "2024-10-16T21:51:30.094081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_random_forest(X_train, y_train):\n",
    "  # Initialize the Random Forest Classifier\n",
    "  rf_classifier = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=20, min_samples_leaf=2, min_samples_split=5, bootstrap=False, class_weight=\"balanced\")\n",
    "\n",
    "  # Train the model on the training data\n",
    "  # Note that random forest is a decision tree (if-else statment on each node), so the data does not have to be scaled\n",
    "  rf_classifier.fit(X_train, y_train)\n",
    "  return rf_classifier\n",
    "\n",
    "def random_forest(df_train):\n",
    "  # Get the params\n",
    "  X = df_train.drop(\"Claim over 1k\", axis=1)\n",
    "  y = df_train[\"Claim over 1k\"]\n",
    "  test_size = 0.2\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = get_train_params(X, y, standard_scaler, test_size)\n",
    "\n",
    "  rf_classifier = run_random_forest(X_train_scaled, y_train)\n",
    "  accuracy, sensitivity, specificity = test_monitor(rf_classifier, X_test_scaled, y_test)\n",
    "\n",
    "  return accuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:30.107457Z",
     "iopub.status.busy": "2024-10-16T21:51:30.106669Z",
     "iopub.status.idle": "2024-10-16T21:51:30.121468Z",
     "shell.execute_reply": "2024-10-16T21:51:30.120259Z",
     "shell.execute_reply.started": "2024-10-16T21:51:30.107409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_iteration(df_train, times, cols=[]):\n",
    "  accuracy, sensitivity, specificity = 0, 0, 0\n",
    "  df_shorten_train = df_train.drop(columns=cols, axis=1)\n",
    "  for _ in range(times):\n",
    "    with SuppressPrints():\n",
    "      accuracy_get, sensitivity_get, specificity_get = random_forest(df_shorten_train)\n",
    "      accuracy += float(accuracy_get)\n",
    "      sensitivity += float(sensitivity_get)\n",
    "      specificity += float(specificity_get)\n",
    "  print(f\"Average over {times} runs: \")\n",
    "  print(f\"Accurarcy: {accuracy / times}\")\n",
    "  print(f\"Sensitivity: {sensitivity / times}\") \n",
    "  print(f\"Specificity: {specificity / times}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:30.124393Z",
     "iopub.status.busy": "2024-10-16T21:51:30.124045Z",
     "iopub.status.idle": "2024-10-16T21:51:32.032820Z",
     "shell.execute_reply": "2024-10-16T21:51:32.031818Z",
     "shell.execute_reply.started": "2024-10-16T21:51:30.124357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random_forest(df_process) # test with no oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:41.863002Z",
     "iopub.status.busy": "2024-10-16T21:51:41.862576Z",
     "iopub.status.idle": "2024-10-16T21:51:43.467036Z",
     "shell.execute_reply": "2024-10-16T21:51:43.465797Z",
     "shell.execute_reply.started": "2024-10-16T21:51:41.862936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"Date Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(df_process.drop(columns=cols_to_drop, axis=1)) # test with removing some cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_iteration(df_process, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tune_random_forest(df_train):\n",
    "  X = df_train.drop(\"Claim over 1k\", axis=1)\n",
    "  y = df_train[\"Claim over 1k\"]\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = get_train_params(X, y, standard_scaler, 0.2)\n",
    "  # Step 2: Define the model\n",
    "  rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "  # Step 3: Specify hyperparameters to tune\n",
    "  param_grid = {\n",
    "    'n_estimators': [100, 200, 250],          # Number of trees\n",
    "    'max_depth': [10, 20, 30],             # Maximum depth of the trees\n",
    "    'min_samples_split': [5, 10],         # Minimum samples to split an internal node\n",
    "    'min_samples_leaf': [5, 10],          # Minimum samples at a leaf node\n",
    "    'bootstrap': [False],\n",
    "    'class_weight': ['balanced']\n",
    "  }\n",
    "\n",
    "  # Step 4: Set up GridSearchCV\n",
    "  grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid,\n",
    "                 cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "  # Step 5: Fit the model\n",
    "  grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "  # Step 6: Evaluate the best model\n",
    "  best_model = grid_search.best_estimator_\n",
    "  \n",
    "  return best_model\n",
    "# print(hyper_tune_random_forest(df_process))\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=30, min_samples_leaf=2,\n",
    "#                        min_samples_split=5, n_estimators=250, random_state=42)\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=30, min_samples_leaf=5,\n",
    "#                        min_samples_split=5, n_estimators=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:51.885575Z",
     "iopub.status.busy": "2024-10-16T21:51:51.885151Z",
     "iopub.status.idle": "2024-10-16T21:51:51.891927Z",
     "shell.execute_reply": "2024-10-16T21:51:51.890710Z",
     "shell.execute_reply.started": "2024-10-16T21:51:51.885525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(df_train, test_size):\n",
    "  df_train.value_counts()\n",
    "  # Get the params\n",
    "  X = df_train.drop(\"Claim over 1k\", axis=1)\n",
    "  y = df_train[\"Claim over 1k\"]\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = get_train_params(X, y, standard_scaler, test_size)\n",
    "\n",
    "  rf_classifier = run_random_forest(X_train_scaled, y_train)\n",
    "  test_monitor(rf_classifier, X_test_scaled, y_test)\n",
    "  return rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:51.896405Z",
     "iopub.status.busy": "2024-10-16T21:51:51.896033Z",
     "iopub.status.idle": "2024-10-16T21:51:53.436872Z",
     "shell.execute_reply": "2024-10-16T21:51:53.435793Z",
     "shell.execute_reply.started": "2024-10-16T21:51:51.896361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train to get the model out\n",
    "my_model = train(df_process, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:53.438576Z",
     "iopub.status.busy": "2024-10-16T21:51:53.438242Z",
     "iopub.status.idle": "2024-10-16T21:51:53.505543Z",
     "shell.execute_reply": "2024-10-16T21:51:53.504390Z",
     "shell.execute_reply.started": "2024-10-16T21:51:53.438540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:53.552355Z",
     "iopub.status.busy": "2024-10-16T21:51:53.551877Z",
     "iopub.status.idle": "2024-10-16T21:51:53.580111Z",
     "shell.execute_reply": "2024-10-16T21:51:53.579053Z",
     "shell.execute_reply.started": "2024-10-16T21:51:53.552306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(folder + \"test.csv\").drop(columns=[\"Coverage\", \"Education\", \"Employment Status\", \"Marital Status\", \"Policy Type\", \"Policy\", \"Sales Channel\", \"Vehicle Size\"])\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:53.581916Z",
     "iopub.status.busy": "2024-10-16T21:51:53.581517Z",
     "iopub.status.idle": "2024-10-16T21:51:53.611442Z",
     "shell.execute_reply": "2024-10-16T21:51:53.610594Z",
     "shell.execute_reply.started": "2024-10-16T21:51:53.581870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(\"CustomerID\", axis=1)\n",
    "X_test = pre_process(X_test)\n",
    "X_test_scaled = normalize(X_test, standard_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:53.612717Z",
     "iopub.status.busy": "2024-10-16T21:51:53.612425Z",
     "iopub.status.idle": "2024-10-16T21:51:53.618398Z",
     "shell.execute_reply": "2024-10-16T21:51:53.617274Z",
     "shell.execute_reply.started": "2024-10-16T21:51:53.612686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X_test):\n",
    "  y_test_pred = model.predict(X_test)\n",
    "  customer_ID = np.array(df_test[\"CustomerID\"])\n",
    "  result = np.column_stack((customer_ID, y_test_pred))\n",
    "  return pd.DataFrame(result).rename(columns={0: \"CustomerID\", 1: \"Claim over 1k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:51:53.619974Z",
     "iopub.status.busy": "2024-10-16T21:51:53.619647Z",
     "iopub.status.idle": "2024-10-16T21:51:53.690913Z",
     "shell.execute_reply": "2024-10-16T21:51:53.689829Z",
     "shell.execute_reply.started": "2024-10-16T21:51:53.619925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_run_test_ouput = predict(my_model, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T21:55:25.473014Z",
     "iopub.status.busy": "2024-10-16T21:55:25.472573Z",
     "iopub.status.idle": "2024-10-16T21:55:25.481735Z",
     "shell.execute_reply": "2024-10-16T21:55:25.480744Z",
     "shell.execute_reply.started": "2024-10-16T21:55:25.472951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_run_test_ouput[\"Claim over 1k\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5875998,
     "sourceId": 9626166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5889994,
     "sourceId": 9644695,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
